Test #1:
I ran the preprocessor code on one of the original IP files,
and found out that the code was not picking up the 
args parameter. It was not available in the resulting json file.
Additionally, the code is mapping the eventIDs to their general
category. The dictionary for this mapping is in the code itself.
This is not very scalable, but since this is a prototype
creation, I can work with that. But I need to make sure that all
eventIDs are mapped in the dictionary.

Test #2:
I got inspired from the SELECT DISTINCT command in SQL, and 
created a nameGetter.py file that will get unique names from
each column. I ran this on the eventNames, to get all the event
names. Then I gave those to grok and told him to create a
generalized dictionary.

Test #3:
Things finally seem to work out. Now the dictionary mapping is
working properly, and the args parameter is in the resulting
set as well. For the args parameter, I gave the code to gpt,
and he told me that it was treating args as json and not a
string, even though all columns were converted into string
at the start of the code. This helped to resolve the issue.
Additionally, when I was looking for a new dataset to test
this code on, I realized that some datasets are already present
for training, testing and validation, by the ratio of:
60:20:20.

Test #4:
I tried to preprocess the training dataset but there are two new
columns named mountNameSpace and StackAddress, which are not 
appearing in the resulting dataset.

Test #5:
The preprocssing module is working perfectly now. It takes every
single field. The problem was very simple, the name of the 
mountNamespace and StackAddresses were not given properly in the
python code. Now that the preprocessing in working fine, I now check 
whether the suspicious attributes of the dataset, mentioned in 
Read_me.txt are required or not.

Test #6
From the design of the research papaer, I have found out that the testing,
and validation datasets also need to be converted into the .json format.
Now I am going to change the file name, in the preprocessor.py script
at line 135, in the input variable. Hope it goes well! Additionally, I am 
going to rename the output file accordingly as well.

Insights: I am currently assuming the suspicious attributes mentioned in test #5, and discussed in Read_me.txt
are currently required for training. Additionally, during test#6, I have doubts about the timestamp field.
What is it showing? It actually shows that when the data collection was started at time 0, at what time did this 
attack occured. For example 129.07 shows that after 129.07 secs, of collecting data, the mentioned attack occured.
Now, I can get following benefits from this field:
1. Model events chronologically, which is essentail for LSTM model of the trio DL.
2. Find attack duration, by grouping attacks on the basis of process, event IDs, 
this way, I can find out the start and end of the attack, by which command it started, 
and by which it ended. This can help in the future.
Currently, this implementation is present on GROK of the mhali... account. To access it, 
go to the AI-driven SOAR System Model chat and ask about timestamp_analyzer.py.

Test #7: (6 Aug 2025)
I applied TF-IDF.py on the generated .json files from test #6.
What this is doing is that it takes the fields:
threat_type, processName, eventName, and the args list.
Additionally, it takes argsNum, and returnValue from the .json files
and vectorizes them in the Vectorized_tfidf folder. Currently, it only takes
500 most frequest words from the .json files. Currently, this is defined
in the apply_tfidf function of TF-IDF.py. This is only for testing the model currenlty,
if it is not sufficient, we will increase the features later. If you ever need to know,
 the possible solution for the max_feature, go to gpt chat: Preprocessing issues, and ask
 it of vectorizing solutions. Hope to implement an AI to decide for this in the future.

 Insights: The result of test #7 will be used in test #8 to create profiles.

 Test #8: (6 Aug 2025)
 event_profiler.py has been run. This took the vectorized data, and got profiles from them.
 Then it took the .json original files and added profiles on them. These profiles are used 
 to give an upper classification to data. Currently the number of profiles is 5. This is default 
 for a small dataset, however if you need to change it in the future, go to gpt chat: Preprocessing issues.
 And ask it about Profiling solutions. We might even train a AI model to do this on it's own in the future.
 The result of test #8 is stored in the AI_SOAR_1/Preprocessing/Profiled folder.

 Insights: Now, that profiling has been completed, I am going to used the profiled data and the vecotorized data
 to train the DL trio. Since each model in the DL trio requires different types of data. I have created
 a data_prepare.py file, which will take the profling and vectorized data and create the desired type of 
 data for each model. This result will be stoerd in DL_Data folder. Insided there are three folders for 
 each of the DL model.

 Test #9: (7 Aug 2025): USIND data_prepare.py
 The data_prepare.py is successfully converting data for FNN and CNN models, but it gets stucked when working of 
 LSTM. I asked gpt and found out that the following line, in the prepare_lstm_data() function:
 
 sorted_indices = [data.index(entry) for entry in data_sorted]

 runs for each line for a single object. If we have n lines in a object and n objects that it is creating a 
 time complexity of n^2. I ran it for almost 30 mins, and there was still not result. Now this line has 
 been replaced by the following line:

 # Create a map of entry ID to index in the original data list
id_map = {id(entry): idx for idx, entry in enumerate(data)}

# Use the map to quickly find each entryâ€™s index
sorted_indices = [id_map[id(entry)] for entry in data_sorted]

Now, it is indexing the objects, which will hopefully work.

Test #10 (7 Aug 2025): Testing the new line in data_prepare.py
The line did worked well, but now we have another error. For the training data, the function generated 
a ver huge array, of the size: 28.5 GB. This could, not be returned by the function.

Possible solution:
1. Return the value from function in the form of chunks, and save them into the original file, by
writing at the end of the file.

Test #11 (for soluiont 1):
The solution was successfull. Now, we have the data for the training of all three models.
Remember that you can use the chunk solution in the other two prepare functions as well.

Test #12:
I am going to check the shape of the .pnpy files for the training of all three models,
through Check_shape.py in DL_data