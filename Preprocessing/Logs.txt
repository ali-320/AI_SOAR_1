Test #1:
I ran the preprocessor code on one of the original IP files,
and found out that the code was not picking up the 
args parameter. It was not available in the resulting json file.
Additionally, the code is mapping the eventIDs to their general
category. The dictionary for this mapping is in the code itself.
This is not very scalable, but since this is a prototype
creation, I can work with that. But I need to make sure that all
eventIDs are mapped in the dictionary.

Test #2:
I got inspired from the SELECT DISTINCT command in SQL, and 
created a nameGetter.py file that will get unique names from
each column. I ran this on the eventNames, to get all the event
names. Then I gave those to grok and told him to create a
generalized dictionary.

Test #3:
Things finally seem to work out. Now the dictionary mapping is
working properly, and the args parameter is in the resulting
set as well. For the args parameter, I gave the code to gpt,
and he told me that it was treating args as json and not a
string, even though all columns were converted into string
at the start of the code. This helped to resolve the issue.
Additionally, when I was looking for a new dataset to test
this code on, I realized that some datasets are already present
for training, testing and validation, by the ratio of:
60:20:20.

Test #4:
I tried to preprocess the training dataset but there are two new
columns named mountNameSpace and StackAddress, which are not 
appearing in the resulting dataset.

Test #5:
The preprocssing module is working perfectly now. It takes every
single field. The problem was very simple, the name of the 
mountNamespace and StackAddresses were not given properly in the
python code. Now that the preprocessing in working fine, I now check 
whether the suspicious attributes of the dataset, mentioned in 
Read_me.txt are required or not.

Test #6
From the design of the research papaer, I have found out that the testing,
and validation datasets also need to be converted into the .json format.
Now I am going to change the file name, in the preprocessor.py script
at line 135, in the input variable. Hope it goes well! Additionally, I am 
going to rename the output file accordingly as well.

Insights: I am currently assuming the suspicious attributes mentioned in test #5, and discussed in Read_me.txt
are currently required for training. Additionally, during test#6, I have doubts about the timestamp field.
What is it showing? It actually shows that when the data collection was started at time 0, at what time did this 
attack occured. For example 129.07 shows that after 129.07 secs, of collecting data, the mentioned attack occured.
Now, I can get following benefits from this field:
1. Model events chronologically, which is essentail for LSTM model of the trio DL.
2. Find attack duration, by grouping attacks on the basis of process, event IDs, 
this way, I can find out the start and end of the attack, by which command it started, 
and by which it ended. This can help in the future.
Currently, this implementation is present on GROK of the mhali... account. To access it, 
go to the AI-driven SOAR System Model chat and ask about timestamp_analyzer.py.

